{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS\n",
    "I chose Russian(rus) as the source language to project the POS to Kazakh (kk) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data\n",
    "I spent a lot time to make the fastalign to work, and then I spent a lot time to find that udpipe model tokenize the original file into weird sentences in conllu format because there are many numbers as separate lines in the original files (ru.s or kk.s) and some sentences starts with a number which udpipe tokenize the number and the rest part of the sentence into two sentences in *conllu*, anyways I make the parallel sentence pairs first using your command line and then filtered all the lines with any numbers or has no paired parallel sentence as following. And I use this ***ru-kk_filtered*** parallel file to extract the source and target sentences again. In the end it results in 43550 sentences for both target and source languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "### get rid of empty alignments\n",
    "lines = []\n",
    "with open(\"./ru-kk\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if re.match(\"([0-9]* \\.)|([0-9]* , [0-9]* \\.)\", line):\n",
    "            #print(line)\n",
    "            continue\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        lines.append(line.strip())\n",
    "        #print(line.strip())\n",
    "with open(\"./ru-kk_filtered\", \"w\", encoding=\"utf-8\") as new_f:  \n",
    "    for line in lines:\n",
    "        print(line, file=new_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./ru-kk\", \"r\", encoding=\"utf-8\") as f:\n",
    "    r_lines, k_lines = [], []\n",
    "    for line in f:\n",
    "        line = line.strip().split(\" ||| \")\n",
    "        r, k = line[0], line[1]\n",
    "        r_lines.append(r)\n",
    "        k_lines.append(k)\n",
    "\n",
    "with open(\"./ru_filtered.s\", \"w\", encoding=\"utf-8\") as r_f:\n",
    "    for line in r_lines:\n",
    "        print(line, end=\"\\n\\n\",file=r_f)\n",
    "with open(\"./kk_filtered.s\", \"w\", encoding=\"utf-8\") as k_f:\n",
    "    for line in k_lines:\n",
    "        print(line, end=\"\\n\\n\", file=k_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get conllu file from filtered source and target language files\n",
    "- Source language (ru): I downloaded udpipe model of russian and run tokenize and tag on the ru_filtered.s file \n",
    "- Target language (kk): I don't understand how we should tokenize the target language to which there is supposed to have no trained tokenizer or tagger or parser due to its low resource. I in the end tokenized it using the russian udpipe model, given that we are going to align their sentences word by word later, it's a not bad idea to use the same tokenizer to tokenize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get alignment file\n",
    "Use the ***ru-kk_filtered*** file to run with the fastalign forward and backward(reversed) and finally get the intersection of the both alignments, saved as ***ru-kk_filtered.i***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the alignment file and the two conllu files and target language file kk_filtered.s to project Russian POS onto Kazakh sentences. \n",
    "I only projected every target word that is not aligned with POS \"NOUN\" because I ran out of time. The projected Kazakh conllu file with POS is saved as ***kk_projected_from_ru.conllu***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At last, I trained a udpipe model using ***kk_projected_from_ru.conllu*** file which is saved as **kk.udpipe** \n",
    "and use the test set downloaded from UD to test the accuracy which is expected to be very low: <br>\n",
    "\n",
    "```\n",
    "\n",
    "Tagging from gold tokenization - forms: 10007, upostag: 30.91%, xpostag: 0.00%, feats: 35.73%, alltags: 0.00%, lemmas: 0.00%\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
