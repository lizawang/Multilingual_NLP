#!/usr/bin/env python3
#coding: utf-8

import sys
from collections import defaultdict

# number of sentences -- in PUD it is always 1000
SENTENCES = 1000

# field indexes
ID = 0
FORM = 1
LEMMA = 2
UPOS = 3
XPOS = 4
FEATS = 5
HEAD = 6
DEPREL = 7

# returns dict[source_id] = [target_id_1, target_id_2, target_id_3...]
# and a reverse one as well
# TODO depending on what type of alignment you use, you may not need to have a list of aligned tokens -- maybe there is at most one, or even exactly one?
def read_alignment(fh):
    line = fh.readline()
    src2tgt = defaultdict(list)
    tgt2src = defaultdict(list)
    for st in line.split():
        (src, tgt) = st.split('-')
        src = int(src)
        tgt = int(tgt)
        src2tgt[src].append(tgt)
        tgt2src[tgt].append(src)
    return (src2tgt, tgt2src)

# returns a list of tokens, where each token is a list of fields;
# ID and HEAD are covnerted to integers and switched from 1-based to 0-based
# if delete_pos=True, then morphological anotation (UPOS, XPOS, FEATS) is stripped
def read_sentence(fh, delete_pos=False):
    sentence = list()
    for line in fh:
        if line == '\n':
            # end of sentence
            break
        elif line.startswith('#'):
            # ignore comments
            continue
        else:
            fields = line.strip().split('\t')
            if fields[ID].isdigit():
                # make IDs 0-based to match alignment IDs
                fields[ID] = int(fields[ID])-1
                # fields[HEAD] = int(fields[HEAD])-1
                if delete_pos:
                    fields[UPOS] = '_'
                    fields[XPOS] = '_'
                    fields[FEATS] = '_'
                sentence.append(fields)
            # else special token -- continue
    return sentence

# takes list of lists as input, ie as returned by read_sentence()
# switches ID and HEAD back to 1-based and converts them to strings
# joins fields by tabs and tokens by endlines and returns the CONLL string
def write_sentence(sentence):
    result = list()
    for fields in sentence:
        # switch back to 1-based IDs
        fields[ID] = str(fields[ID]+1)
        # fields[HEAD] = str(fields[HEAD]+1)
        result.append('\t'.join(fields))
    result.append('')
    return '\n'.join(result)

def main(source_filename, target_filename, alignment_filename):
    with open(source_filename, "r", encoding="utf-8") as source, open(target_filename, "r", encoding="utf-8") as target, open(alignment_filename, "r", encoding="utf-8") as alignment:
        all_target_sentences = []
        for sentence_id in range(SENTENCES):
            (src2tgt, tgt2src) = read_alignment(alignment)
            source_sentence = read_sentence(source)
            # target language supposed to not have any tools to tokenize it and write into conllu file??
            # I used the udpipe model of source language to tokenize the target language
            target_sentence = read_sentence(target, delete_pos=True)
        
            # TODO do the projection
            # iterate over source tokens
            # TODO maybe you want to iterate over target tokens?
            for target_token in target_sentence:
                target_token_id = target_token[ID]
                if target_token_id in tgt2src:
                    src_token_id_list = tgt2src[target_token_id]
                    #print(sentence_id, src_token_id_list, source_sentence)
                    for src_token_id in src_token_id_list:
                        # pass
                        # TODO copy source UPOS to target UPOS?
                        target_sentence[target_token_id][UPOS] = source_sentence[src_token_id][UPOS]
                else:
                    target_sentence[target_token_id][UPOS] = "NOUN"
                
    
            final_target_sentence = write_sentence(target_sentence)
            all_target_sentences.append(final_target_sentence)
        return all_target_sentences
    
if __name__ == "__main__":
    # parameters
    source_filename, target_filename, alignment_filename = "ru_filtered.conllu", "kk_filtered.conllu", "ru-kk_filtered.i"
    target_sentences_fields = main(source_filename, target_filename, alignment_filename)
    
    with open("kk_filtered.s", "r", encoding="utf-8") as f, open("kk_projected_from_ru.conllu", "w", encoding="utf-8") as out_f:
        text_lines = f.readlines()
        sent_fields = target_sentences_fields[0]
        text = text_lines[0].strip()
        print("# sent_id = 1" , file=out_f)
        print("# text = " + text, file=out_f)
        print(sent_fields, file=out_f)
        j = 0
        for sent_id in range(SENTENCES):
            sent_fields = target_sentences_fields[sent_id+1]
            # because the file has each line separated with an empty line
            i =  j + 2
            text = text_lines[i] 
            j =  i
            print("# sent_id = " + str(sent_id+1) , file=out_f)
            print("# text = " + text, file=out_f)
            print(sent_fields, file=out_f)
            
            

