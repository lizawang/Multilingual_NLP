{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### About getting the alignments\n",
    "The problem of English and Chinese is that they are not one word to one charactar mapping, I try to get rid of all multiple-words English phrases in the .dic file into single English words. I hope this dictionary at least helps a bit in training.\n",
    "- I tried to use the parallel data from UPUS and run it with Fastalign to get the intersections, but the results is really bad. Again because English to Mandarin is mostly one to many mapping (but how many? That varies.)\n",
    "- So instead I went back to the following method, very problematic but still it's able to abstract a lot of good alignments. The idea is just when there is many to many mapping, I divided the them into one-two mapping because usually Chinese \"words\" are two characters. The problem is in this way a lot of many (english) to two/three (chinese characters) are mix aligned.\n",
    "\n",
    "(Alignments command lines: <br>\n",
    "\n",
    "```\n",
    "paste en-zh.en en-zh.zh | sed 's/\\t/ ||| /' | grep '. ||| .' > en-zh <br>\n",
    "fast_align/build/fast_align -d -o -v -i en-zh > en-zh.f <br>\n",
    "fast_align/build/fast_align -d -o -v -r -i en-zh > en-zh.r <br>\n",
    "fast_align/build/atools -i en-zh.f -j en-zh.r -c intersect > en-zh.i <br>\n",
    "```\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lines = []\n",
    "with open(\"en-zh.dic\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split()\n",
    "        line = line[2:-2]\n",
    "        if len(line) > 2:\n",
    "            en = line[:-1]\n",
    "            zh = line[-1]\n",
    "            for w in en:\n",
    "                #print(w)\n",
    "                if len(zh)>=2:\n",
    "                    pair = w + \" \" + zh[:2]\n",
    "                    zh = zh[2:]\n",
    "                    new_lines.append(pair)\n",
    "                elif zh:\n",
    "                    pair = w + \" \" + zh\n",
    "                    new_lines.append(pair)\n",
    "        elif len(line) == 2 and line[0]!=\"\" and line[1] !=\"\":\n",
    "            new_lines.append(line[0] + \" \" + line[1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['world 世界',\n",
       " 'order 秩序',\n",
       " 'world 世界',\n",
       " 's 领导',\n",
       " 'leaders 人',\n",
       " 'would 那样',\n",
       " 'most 辉煌',\n",
       " 'writers 作家',\n",
       " 'wrong 大错特错了',\n",
       " 'year 一年',\n",
       " 'ago 前',\n",
       " 'years 几年',\n",
       " 'ago 前',\n",
       " 'yellow 黄祸',\n",
       " 'yen 日元',\n",
       " 'yes 是的',\n",
       " 'yes 没错',\n",
       " 'young 年轻',\n",
       " 'people 人',\n",
       " 'Šefčovič Šefčovič']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lines[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"en-zh_train.dic\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in new_lines:\n",
    "        print(line, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now you run the new dictionary file with the *en* and *zh* vectors over *vecmap* (supervised) and get the crosslingual embeddings from both sides. I took only top 100000 vectors and save them into python dictionaries.\n",
    "Command line to get crosslingual embeddings: <br>\n",
    "\n",
    "```\n",
    "python vecmap/map_embeddings.py --supervised en-zh_train.dic cc.en.300.vec cc.zh.300.vec en_mapped.emb zh_mapped.emb\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000 300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zh_embeds, en_embeds = {}, {}\n",
    "max_tokens = 100000\n",
    "with open(\"zh_mapped.emb\", \"r\", encoding=\"utf-8\") as f, open(\"en_mapped.emb\", \"r\", encoding=\"utf-8\") as f2:\n",
    "    i, j = 0, 0\n",
    "    for line in f:\n",
    "        line = line.strip().split()\n",
    "        zh_embeds[line[0]] = [float(n) for n in line[1:]]\n",
    "        i += 1\n",
    "        if i == max_tokens:\n",
    "            break\n",
    "    for line in f2:\n",
    "        line = line.strip().split()\n",
    "        en_embeds[line[0]] = [float(n) for n in line[1:]]\n",
    "        j += 1\n",
    "        if j == max_tokens:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### And then concatenate the two crosslingual embedding sets (remove some punctuation duplicates). And print them into one embedding file called \"final....\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = en_embeds\n",
    "for token in zh_embeds:\n",
    "    if token not in en_embeds:\n",
    "        merged[token] = zh_embeds[token]\n",
    "len(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final_en-zh_embeds.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    print(len(merged), sep=\"\", end=\" \", file=f)\n",
    "    print(300, sep=\"\", file=f)\n",
    "    for pair in merged.items():\n",
    "        token = pair[0]\n",
    "        embed = pair[1]\n",
    "        print(token, sep=\"\", end=\" \", file=f)\n",
    "        print(embed, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally train the en udpipe model using the final merged embedding file using the ud English test set from UD. And evaluate the trained udpipe model on the testset of Chinese from UD.\n",
    "- The result is really bad?! Maybe the dict file to vecmap is not good (not helpful) or is it the udpipe model training set too small because I used only the en UD test set.\n",
    "\n",
    "**THE RESULT** is as following:<br>\n",
    "Parsing from gold tokenization with gold tags - forms: 12012, **UAS: 29.44%, LAS: 18.32%**\n",
    "\n",
    "Command line: <br>\n",
    "\n",
    "```\n",
    "../assignment03/udpipe --train en.udpipe --tokenizer=none --tagger=none --parser='embedding_form_file=final_en-zh_embeds.txt' en_ewt-ud-test.conllu\n",
    "\n",
    "../assignment03/udpipe --parse --accuracy en.udpipe zh_gsdsimp-ud-test.conllu \n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Lets try to compare it with the delexed model trained with the same source data and evaluate on the same target data.\n",
    "\n",
    "**SO!** The delexed parser worked even much better than the one with embeddings?! What did I do wrong?\n",
    "\n",
    "**The RESULT:** <br>\n",
    "Parsing from gold tokenization with gold tags - forms: 12012, **UAS: 36.97%, LAS: 26.62%**\n",
    "\n",
    "Command line: \n",
    "\n",
    "```\n",
    "../assignment03/udpipe --train en_delex.udpipe --tokenizer=none --tagger=none --parser='embedding_form=0;embedding_feats=0\n",
    ";' en_ewt-ud-test.conllu \n",
    "\n",
    "../assignment03/udpipe --parse --accuracy en_delex.udpipe zh_gsdsimp-ud-test.conllu  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
